{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Retrieval Augmented Generation (RAG) from Scratch"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "281393f3fdcea41f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "When LLMs are created, they are trained on a fixed dataset, which means they only know information available up to the time of training. But what if we want to talk to an LLM about new or ever-changing information that wasn't in their original dataset? Or what about private data? This is where RAG comes in. Using this approach we can pass relevant and up-to-date information from external sources to the LLM in real time before generating a response. Let's see how it's done...\n",
    "\n",
    "![Retrieval](retrieval.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a6ad84acf546ab7"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAQ: What is your return policy? You can return any item within 30 days of purchase for a full refund, provided it is in its original condition.\n",
      "Token Count: 34\n",
      "\n",
      "FAQ: Do you offer free shipping? Yes, we offer free standard shipping on orders over $50.\n",
      "Token Count: 21\n",
      "\n",
      "FAQ: How can I track my order? Once your order is shipped, you will receive an email with a tracking number and a link to track your package.\n",
      "Token Count: 34\n",
      "\n",
      "FAQ: Can I change or cancel my order? You can change or cancel your order within 24 hours of placing it by contacting our customer service.\n",
      "Token Count: 33\n",
      "\n",
      "FAQ: What payment methods do you accept? We accept Visa, MasterCard, American Express, PayPal, and Apple Pay.\n",
      "Token Count: 26\n",
      "\n",
      "FAQ: Do you have a physical store location? Yes, we have several store locations. Please visit our website to find the nearest store.\n",
      "Token Count: 32\n",
      "\n",
      "FAQ: How do I use a promo code? Enter your promo code at checkout in the designated field to apply the discount.\n",
      "Token Count: 26\n",
      "\n",
      "FAQ: What should I do if I receive a damaged item? Please contact our customer service immediately with your order number and a photo of the damaged item.\n",
      "Token Count: 37\n",
      "\n",
      "FAQ: Do you offer gift wrapping services? Yes, we offer gift wrapping services for a small additional fee at checkout.\n",
      "Token Count: 28\n",
      "\n",
      "FAQ: How can I contact customer service? You can contact our customer service via email, phone, or live chat on our website.\n",
      "Token Count: 29\n",
      "\n",
      "FAQ: What is your exchange policy? Exchanges are allowed within 30 days of purchase for items in their original condition.\n",
      "Token Count: 29\n",
      "\n",
      "FAQ: Do you offer international shipping? Yes, we ship to select international locations. Shipping fees and delivery times vary by destination.\n",
      "Token Count: 34\n",
      "\n",
      "FAQ: How do I create an account? Click on the 'Sign Up' button on our website and fill in the required information to create an account.\n",
      "Token Count: 32\n",
      "\n",
      "FAQ: Can I purchase a gift card online? Yes, you can purchase both physical and digital gift cards on our website.\n",
      "Token Count: 27\n",
      "\n",
      "FAQ: What is your price match policy? We will match the price of any identical item found at a lower price from a competitor.\n",
      "Token Count: 30\n",
      "\n",
      "FAQ: How do I subscribe to your newsletter? Enter your email address in the subscription box at the bottom of our website to receive our newsletter.\n",
      "Token Count: 35\n",
      "\n",
      "FAQ: What should I do if I forget my password? Click on 'Forgot Password' on the login page and follow the instructions to reset your password.\n",
      "Token Count: 34\n",
      "\n",
      "FAQ: Do you offer a loyalty program? Yes, we offer a loyalty program that rewards points for every purchase, which can be redeemed for discounts.\n",
      "Token Count: 35\n",
      "\n",
      "FAQ: How can I find out about sales and promotions? Sign up for our newsletter or follow us on social media to stay updated on sales and promotions.\n",
      "Token Count: 35\n",
      "\n",
      "FAQ: What is your policy on defective products? If you receive a defective product, please contact our customer service for a replacement or refund.\n",
      "Token Count: 35\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_faqs(path: str) -> list[dict]:\n",
    "    \"\"\"Loads FAQs from a JSON file and adds a token count to each.\"\"\"\n",
    "    with open(path, \"r\") as file:\n",
    "        file = json.load(file)\n",
    "    qas = []\n",
    "    for qa in file:\n",
    "        faq_text = f\"{qa['question']} {qa['answer']}\"\n",
    "        token_count = len(faq_text) // 4 # Calculate token count for the combined string. One token is roughly 4 english characters\n",
    "        qas.append({'faq': faq_text, 'token_count': token_count})\n",
    "    return qas\n",
    "\n",
    "\n",
    "# Load FAQs from JSON file\n",
    "faq_path = \"faq.json\"\n",
    "faqs = load_faqs(faq_path)\n",
    "\n",
    "# Print the FAQs\n",
    "for faq in faqs:\n",
    "    print(f\"FAQ: {faq['faq']}\")\n",
    "    print(f\"Token Count: {faq['token_count']}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-20T10:26:50.352543Z",
     "start_time": "2024-11-20T10:26:50.347640Z"
    }
   },
   "id": "60c0dbda4a03a906",
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "b3cb48f5ee0a2759",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Why do we care about token count?\n",
    "\n",
    "Token count is important to think about because:\n",
    "1. Embedding models can't ingest an infinite amount of tokens (more on this soon)\n",
    "2. LLM's also can't ingest an infinite amount of tokens (context window)\n",
    "\n",
    "For instance, an embedding model will be trained to take an input of _x_ amount of tokens embed it into numerical space.\n",
    "If the token count of the thing we want to embed is larger than what the model supports, we must split it up into chunks.\n",
    "In our case, we don't need to worry about this today. But let's say you're working with a large PDF, this will be something to take into account.\n",
    "In the case of an LLM, GPT-4o for example has a max context window (input) of 128,000 tokens, so around 512,000 characters, and a max output length of 16,384 tokens.\n",
    "\n",
    "https://platform.openai.com/tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0435c91af89230",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Processing each FAQ into an embedding\n",
    "\n",
    "Okay, let's embed each FAQ into its own numerical representation, but what does this even mean? And why are we doing it?\n",
    "\n",
    "While humans understand language and text, computers, as we all know, understand numbers. Let's provide it with some...\n",
    " \n",
    "To \"embed\" a piece of text means to transform it into a vector representation within a high-dimensional space. \n",
    "\n",
    "\"_vectors_, _high-dimensional spaces_, _what!?_\"\n",
    "\n",
    "- A **vector** is a mathematical object that has both magnitude and direction, often represented as an ordered list of numbers corresponding to coordinates in a space. In simpler terms, it's an array of numbers, e.g [3, 4, 5]\n",
    "- A **high-dimensional** space refers to a mathematical space with many dimensions, allowing it to capture complex relationships and semantic similarities between it's data points. Whereas we work in the three dimensions, these spaces can have thousands, making it impossible to intuitively comprehend. In said spaces, ***semantically*** similar data points are positioned closer to each other. For example, the numerical values for \"Dog\" and \"Cat\" might be positioned fairly close to each other, and likewise the numerical representation for the word \"Bank\" might be equally positioned between regions representing Finance and Geography. \n",
    "\n",
    "But which numbers do we pick...? And how do we \"encode\" the semantic meaning behind a given piece of text into these numbers?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Embedding Models\n",
    "\n",
    "Embedding models are machine learning models designed to convert data (like words or images) into vectors. They are trained using large datasets where the model adjusts the vector representations to minimize the distance between similar items and maximize the distance between dissimilar ones. So to start off, the vector representations are completely random. But as training progresses, the model starts to learn the similarities and differences between various inputs, and generates the appropriate vector in response. Let's take a look:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0dbadbfe9a26e18"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: This is for a tech talk\n",
      "Embedding: [-1.85906217e-02 -7.89896250e-02 -2.90067401e-02 -1.14555974e-02\n",
      " -2.81989183e-02  1.18537704e-02  5.35872839e-02 -5.07863499e-02\n",
      " -2.27182433e-02  2.66975537e-03  6.44041747e-02 -3.70712988e-02\n",
      "  1.86049100e-02  6.85815886e-02  3.68726323e-03 -5.77761009e-02\n",
      "  3.97582017e-02  1.33974403e-02 -2.54959092e-02  5.75061962e-02\n",
      " -1.32698547e-02  1.59527957e-02 -3.02311834e-02 -1.65105832e-03\n",
      " -5.11745829e-03 -1.42291067e-02 -1.74166542e-02  8.79486650e-03\n",
      " -7.65029900e-03  2.66950345e-04  5.61381807e-04 -5.49411364e-02\n",
      " -6.56222051e-04 -5.15956096e-02  1.56339365e-06  1.01803159e-02\n",
      " -2.44737752e-02  9.87703353e-03 -7.02290237e-02  9.24384445e-02\n",
      "  4.40662820e-03  6.48789927e-02  1.49574815e-04  7.06449598e-02\n",
      "  5.08026183e-02 -3.47991362e-02  6.47578314e-02  3.67464758e-02\n",
      "  6.07231911e-03  6.80860737e-03 -6.64794957e-03  1.38000473e-02\n",
      " -5.59679233e-03 -2.69164629e-02 -6.31440580e-02 -4.03364561e-03\n",
      " -3.17131057e-02  2.97694094e-02  2.82389633e-02  1.93557162e-02\n",
      "  4.17129733e-02 -2.09888872e-02 -1.40659139e-02 -2.82733813e-02\n",
      "  1.85071409e-03  3.50792930e-02  6.21692697e-03 -4.09462005e-02\n",
      " -3.01208869e-02  2.31994297e-02  2.03519687e-02 -5.58228057e-04\n",
      "  1.82350129e-02  7.39466818e-03  6.29170798e-03 -1.28820688e-02\n",
      " -2.71103028e-02 -3.24380957e-02 -4.05693687e-02  2.23481245e-02\n",
      " -3.02884839e-02  6.86677322e-02  1.63792148e-02 -4.28055637e-02\n",
      "  6.18114397e-02  2.58537419e-02 -1.34026771e-02  1.34836929e-03\n",
      " -6.84386417e-02 -8.99619889e-03  9.21037197e-02  2.62617785e-02\n",
      "  5.18735200e-02 -7.85805564e-03 -1.42664760e-02 -1.26755461e-02\n",
      "  5.85629838e-03 -2.79760221e-03 -1.85580030e-02 -5.74040078e-02\n",
      "  3.16248916e-04 -3.68333310e-02  1.62106622e-02  4.10514772e-02\n",
      " -2.34845579e-02 -4.33038548e-02 -2.84906607e-02 -6.54784963e-02\n",
      " -4.93667535e-02 -5.06145740e-03  4.86979447e-02 -4.05705608e-02\n",
      " -4.98279259e-02  1.57953538e-02  2.17495691e-02 -1.92226637e-02\n",
      "  1.63016014e-03  5.31416386e-02 -2.50058039e-03 -1.07327700e-02\n",
      " -1.13330679e-02  1.92841515e-02  1.32638318e-02  8.32910277e-03\n",
      "  1.87806860e-02  1.63025055e-02 -3.57499905e-02 -1.44203389e-02\n",
      "  4.64894921e-02 -4.83751521e-02 -1.36241373e-02 -4.47829030e-02\n",
      "  2.06091199e-02  1.78695265e-02  4.25577117e-03  4.76817079e-02\n",
      "  1.31031731e-02  2.77690459e-02 -6.93318918e-02  7.96175376e-03\n",
      "  4.13874537e-02  1.86019298e-02 -1.60356537e-02  3.36813144e-02\n",
      " -3.24534252e-02  3.45065095e-03  1.16560208e-02  2.46413425e-02\n",
      " -1.39831836e-05  1.54419672e-02 -4.27641235e-02 -2.69795507e-02\n",
      " -3.81879918e-02  5.05634677e-03  5.07194661e-02 -4.87598171e-03\n",
      "  3.26830409e-02 -2.73614042e-02 -8.37126896e-02 -1.10553866e-02\n",
      "  3.02279703e-02 -3.61471772e-02 -2.34503448e-02 -6.43273257e-03\n",
      " -9.84227471e-03  1.95854646e-03  1.59084201e-02 -3.11675016e-03\n",
      " -2.15751212e-02 -1.96496695e-02 -2.81566079e-03  3.06627061e-03\n",
      " -2.20665080e-03  7.59401619e-02  8.55150167e-03  1.09022297e-02\n",
      "  6.13281317e-02 -2.34964807e-02 -3.06230448e-02  1.29798716e-02\n",
      " -1.57893579e-02 -1.17610499e-01  1.29085835e-02  9.53881741e-02\n",
      "  1.50772119e-02  3.10823433e-02  5.73574565e-02 -1.76235680e-02\n",
      " -6.56372830e-02 -1.24916751e-02 -7.23387534e-03 -1.99560691e-02\n",
      "  9.89659876e-03  2.42355503e-02 -3.12306378e-02 -2.01672539e-02\n",
      " -5.11699310e-03  2.82933097e-02 -1.45784412e-02 -6.10653199e-02\n",
      " -3.36861983e-02 -7.16868415e-02 -2.88296007e-02  7.60085359e-02\n",
      "  4.32905136e-03  7.32813030e-03 -3.44634708e-03 -6.39778655e-03\n",
      " -2.04610899e-02 -7.48649314e-02  7.31236488e-02  1.75553225e-02\n",
      " -6.87559918e-02 -8.07689968e-03  2.37826556e-02  2.30644140e-02\n",
      "  1.29206814e-02  2.48833373e-02  4.62791175e-02  9.34723299e-03\n",
      " -1.89967304e-02 -1.93277635e-02 -2.39742380e-02 -4.10619080e-02\n",
      "  5.46161793e-02 -1.19809024e-02 -2.73345644e-03 -2.24922691e-02\n",
      " -4.59540896e-02  2.18104124e-02 -3.89066618e-03  9.57467873e-03\n",
      "  9.92595591e-03  5.09986244e-02 -3.34172100e-02  2.77297366e-02\n",
      "  2.20792759e-02  9.82566774e-02  5.31359809e-03 -2.84415651e-02\n",
      " -1.62049569e-02  3.86894345e-02 -1.98655878e-03  1.71247665e-02\n",
      "  7.02986540e-03 -5.54940924e-02 -6.27961429e-03  1.89974867e-02\n",
      "  1.46454638e-02  1.54752098e-02 -3.05706020e-02  5.52793853e-02\n",
      " -2.42525209e-02  4.22504693e-02 -1.52059048e-02  8.94195810e-02\n",
      "  7.48270703e-03  1.44533236e-02 -7.47250915e-02  1.59522034e-02\n",
      "  7.11799636e-02 -5.12656532e-02  1.91788841e-02  2.42919587e-02\n",
      " -5.86327091e-02 -1.04522482e-02  5.02297245e-02 -6.31384924e-02\n",
      " -4.44583893e-02 -8.18049628e-03 -9.09679756e-03 -9.17749293e-03\n",
      " -2.36828331e-04 -2.70413402e-02 -1.76958852e-02 -3.22689526e-02\n",
      " -5.68037527e-03 -1.25317266e-02 -3.57533433e-02  1.94543786e-02\n",
      " -8.39112047e-03 -6.51830137e-02  5.71717694e-03 -3.27177346e-02\n",
      "  2.64749732e-02  2.99646682e-03  6.44326359e-02 -5.91423810e-02\n",
      " -5.74721172e-02  2.78408993e-02  6.74051046e-02 -2.40903478e-02\n",
      "  4.16546501e-02  5.44428779e-03 -3.22530009e-02  2.51882039e-02\n",
      " -1.52767473e-03  1.46023305e-02  8.42987839e-03  5.76646924e-02\n",
      " -2.77015958e-02  3.45656602e-03  2.90072872e-03  2.45785434e-02\n",
      " -5.60701406e-03  3.63944471e-02  3.96728031e-02 -7.06257895e-02\n",
      "  2.84699108e-02  1.34457564e-02  2.88411998e-03  2.41432190e-02\n",
      " -1.15840472e-02 -2.85668336e-02 -1.42741632e-02 -8.78094882e-03\n",
      "  6.26502745e-03  9.55713633e-03 -4.73506860e-02  2.95477416e-02\n",
      " -1.14068929e-02  1.15164742e-02  5.52904001e-03 -2.15240456e-02\n",
      "  2.60765273e-02  3.23631465e-02 -5.25149889e-02 -2.67034397e-02\n",
      "  3.67845455e-03 -1.10139966e-01  8.46385583e-03  1.68386847e-02\n",
      " -4.75815684e-02  1.21187918e-01 -1.75096411e-02  6.14712350e-02\n",
      " -2.56599858e-02 -4.25797962e-02 -4.62337956e-02 -1.76358726e-02\n",
      " -2.34773122e-02  3.96467410e-02  5.00628538e-02 -5.65911178e-03\n",
      "  4.53992747e-02 -5.25033809e-02  3.56931537e-02  3.16654108e-02\n",
      " -4.50321026e-02 -2.35663317e-02  6.96671605e-02  5.72562143e-02\n",
      " -7.01786345e-03 -2.34988425e-02  3.12165357e-02 -3.43298055e-02\n",
      " -5.25870174e-02 -5.19664260e-03 -4.42824624e-02  1.67655628e-02\n",
      "  1.25257112e-02  1.45032145e-02  2.36370843e-02 -2.66383514e-02\n",
      " -4.92617972e-02 -2.54945662e-02  3.45260128e-02  3.09747420e-02\n",
      " -1.44861108e-02  1.28542492e-02  3.76353264e-02  6.93512261e-02\n",
      "  4.18908298e-02 -2.14756783e-02 -5.96481599e-02  1.14696305e-02\n",
      "  1.16363969e-02  8.29365011e-03  1.34726921e-02  2.79926863e-02\n",
      " -2.05196142e-02 -1.92838609e-02  2.93945652e-02  5.91893960e-03\n",
      "  3.19877975e-02  1.07439840e-02 -5.70836151e-03 -3.92476050e-03\n",
      "  1.10716168e-02 -9.64705460e-03  1.30484905e-02 -3.56638571e-03\n",
      " -5.12720235e-02  4.68938872e-02 -6.36112271e-03 -1.10770809e-02\n",
      "  8.68158042e-02 -3.33384089e-02 -3.55820507e-02 -1.13437073e-02\n",
      "  7.69452900e-02 -1.37291953e-01  9.40121431e-03 -1.49207441e-02\n",
      "  2.14952845e-02  2.17608903e-02  2.03851182e-02 -1.09190973e-04\n",
      " -5.13307527e-02 -6.23362092e-03 -2.06894036e-02 -4.18609288e-03\n",
      "  4.04597037e-02  4.56286082e-03  2.85480004e-02 -3.85741587e-03\n",
      " -9.49680433e-03  4.94196564e-02  4.36849296e-02  3.68861109e-02\n",
      " -7.56609812e-03  1.39226997e-02 -6.29604654e-03 -1.63597502e-02\n",
      " -2.48378259e-03 -2.62214988e-02 -2.91244481e-02  6.11535646e-03\n",
      " -9.93104652e-03 -1.40779151e-03 -3.26079093e-02 -2.72256117e-02\n",
      "  1.80536583e-02  9.34776291e-03  4.41292906e-03 -4.14266512e-02\n",
      " -6.62652403e-02  2.77454369e-02 -2.86453962e-02  2.21441779e-02\n",
      "  8.39263760e-03 -6.49512932e-02  3.36293168e-02  3.20088156e-02\n",
      " -4.34963740e-02 -4.65241075e-03  5.22346832e-02 -2.56529823e-02\n",
      " -1.80233433e-03  7.28850672e-03  6.39888924e-03 -2.35084482e-02\n",
      " -1.28026167e-02 -2.64768675e-02  4.14191782e-02 -1.98122710e-02\n",
      " -2.30575800e-02  3.81475613e-02 -1.39624784e-02 -1.67451389e-02\n",
      " -6.62786094e-03  3.77444550e-02 -6.19896501e-02  4.09050994e-02\n",
      "  4.46917824e-02 -3.16737965e-02 -1.31736137e-02  7.02520926e-03\n",
      " -2.52607595e-02  3.16487215e-02 -4.86558340e-02  1.26194849e-03\n",
      " -3.03310417e-02 -3.56878564e-02 -3.41381691e-02 -4.76147272e-02\n",
      " -1.35059552e-02  1.89607462e-03 -4.91812564e-02  1.47947343e-02\n",
      " -2.75261025e-03 -3.23359147e-02 -2.96778530e-02 -3.47680748e-02\n",
      "  1.26531189e-02  2.14054193e-02  3.28852772e-03  1.76040605e-02\n",
      "  5.44822142e-02 -8.18098150e-03  1.68301212e-03 -4.21696715e-02\n",
      "  4.83031906e-02  1.79951526e-02  1.26383277e-02  1.92114688e-03\n",
      "  2.08269916e-02  1.81020529e-03 -2.89596226e-02 -7.20701006e-04\n",
      " -3.03997584e-02  1.30851846e-02 -3.27630006e-02 -2.05285065e-02\n",
      " -5.65116331e-02 -2.64220741e-02 -2.77032927e-02  3.32485847e-02\n",
      "  6.85937749e-03  4.46512513e-02  3.21925879e-02  1.64421797e-02\n",
      " -6.05448848e-03 -1.53867090e-02  5.08691035e-02 -8.18988716e-04\n",
      "  4.27677436e-03  2.15042718e-02  4.45582811e-03 -2.49668192e-02\n",
      "  3.10956370e-02 -5.04254885e-02  7.55545348e-02 -3.16140130e-02\n",
      "  4.13130298e-02 -5.81859760e-02  9.90096829e-04  9.69362725e-03\n",
      " -2.16530636e-02  3.31838094e-02 -1.63496435e-02 -6.90516606e-02\n",
      " -7.21985400e-02  1.22019760e-02  2.30486831e-03  1.70862246e-02\n",
      "  6.33883104e-02  3.73422317e-02  1.63039695e-02 -4.77989204e-02\n",
      "  1.25969555e-02 -6.16983837e-03  2.68554743e-02 -1.02859791e-02\n",
      "  2.14352161e-02 -2.62683928e-02  4.31637876e-02  5.73148206e-02\n",
      "  8.19052383e-03 -2.25886190e-03  1.34085072e-02  2.66485568e-02\n",
      " -6.21052049e-02  4.52739373e-02 -2.00221371e-02 -5.61548198e-33\n",
      "  9.26557183e-03 -4.73607741e-02  1.43589349e-02  1.08363442e-01\n",
      " -4.81925253e-03  7.46529177e-03  1.39495442e-02  4.38085161e-02\n",
      " -6.46463856e-02  5.88267064e-03 -3.67390960e-02 -7.99691118e-03\n",
      "  3.69824804e-02  6.45129196e-03  5.65355830e-03 -8.95815529e-03\n",
      "  4.98788469e-02  2.60795821e-02  2.08775960e-02  4.24377285e-02\n",
      " -4.23207507e-02 -4.14562300e-02 -2.94254534e-02  4.04218212e-02\n",
      "  8.43925998e-02 -5.73842134e-03 -2.45270407e-04 -3.43258791e-02\n",
      "  2.82636937e-02 -7.45583698e-02  2.60412227e-02  1.30183036e-02\n",
      " -8.33066087e-03 -3.84497829e-02  1.04251187e-02  1.33642897e-01\n",
      " -2.51976456e-02 -1.67119503e-02  3.24206874e-02  2.80557908e-02\n",
      " -3.00967619e-02 -5.08707315e-02 -8.24852474e-03  1.60289127e-02\n",
      " -4.02961448e-02 -3.59452032e-02 -4.85353358e-03  1.07244495e-02\n",
      " -1.15600834e-02 -2.33785540e-04  5.36913462e-02 -1.90286934e-02\n",
      " -1.87667310e-02  6.11383356e-02  3.05945221e-02  7.46481344e-02\n",
      " -3.06767505e-02 -4.83904704e-02  9.63293947e-03  3.24340537e-02\n",
      "  6.49560243e-02  2.81708129e-03  2.47808825e-02 -4.81483564e-02\n",
      " -2.88726874e-02  1.30872410e-02 -4.08945605e-02 -3.57667170e-02\n",
      "  1.92610938e-02 -4.46644053e-03 -2.44307630e-02  8.27834234e-02\n",
      "  1.90867148e-02  3.54898348e-02 -1.18912291e-02 -9.65618994e-03\n",
      "  1.15759298e-02  7.35106831e-03 -1.05313966e-02 -5.41884229e-02\n",
      "  1.76041164e-02 -1.52892657e-02 -1.32201221e-02 -3.21728326e-02\n",
      "  1.31125720e-02 -1.34144919e-02 -4.38109599e-03 -5.57225011e-02\n",
      "  2.62576179e-03  1.31067575e-03 -2.15811729e-02  8.75996202e-02\n",
      " -1.58468541e-03 -3.29895969e-03  5.16703762e-02  6.58411682e-02\n",
      "  8.10366720e-02  1.12540685e-02 -3.63879241e-02 -9.75149423e-02\n",
      " -5.24843037e-02 -1.67901665e-02 -3.18505652e-02  5.00873327e-02\n",
      "  4.03732695e-02 -3.55792455e-02 -3.69829200e-02  2.35629245e-03\n",
      " -5.20051643e-02  8.36787652e-03  2.55841855e-02  2.19104439e-02\n",
      " -7.13822842e-02 -5.60216717e-02 -1.64588522e-02  1.48890764e-02\n",
      "  3.58496606e-02 -7.75522506e-03 -6.44184500e-02 -6.40380159e-02\n",
      " -2.01350451e-02 -3.88579369e-02 -4.85143140e-02  1.92873627e-02\n",
      " -1.28739048e-02  3.47310901e-02 -4.37310562e-02  2.86107361e-02\n",
      "  3.66173498e-02  1.14829037e-02 -2.39939746e-02  1.50738340e-02\n",
      "  2.11810161e-07 -2.35397555e-02  2.15358790e-02  7.49565288e-02\n",
      "  6.24157079e-02  3.69185116e-03  5.59990900e-03  3.10351085e-02\n",
      " -9.78747569e-03  6.94473013e-02 -2.43914239e-02 -3.18366587e-02\n",
      "  3.91150229e-02 -7.38036633e-03  6.26648823e-03 -7.88097158e-02\n",
      " -3.89669500e-02 -6.70045763e-02 -7.31930556e-03  8.02448913e-02\n",
      " -1.60782784e-02  1.14792772e-01  2.60330848e-02  4.91733290e-03\n",
      " -1.46711515e-02  3.60063743e-04 -5.83294481e-02 -5.18073794e-03\n",
      " -2.11079661e-02  2.69314460e-02  1.19258426e-02 -2.41194237e-02\n",
      "  2.00088713e-02  2.46260930e-02  6.83581308e-02  1.18324691e-02\n",
      " -8.05086941e-02  5.13171824e-03  2.84491908e-02 -3.20425606e-03\n",
      " -1.82265788e-02 -2.03159116e-02 -4.63662930e-02 -3.85703519e-02\n",
      "  2.05650013e-02  2.74907742e-02  4.71488573e-02  2.38626245e-02\n",
      "  1.76860169e-02  2.97063179e-02 -1.84385907e-02 -2.29343865e-02\n",
      "  2.26504561e-02  4.96433005e-02 -6.38280809e-02  3.95393968e-02\n",
      " -2.21364852e-02 -3.10625453e-02  2.59373803e-02 -4.23381105e-03\n",
      "  2.50519738e-02 -1.43163987e-02 -3.87606286e-02  2.67882664e-02\n",
      "  3.08324350e-03 -6.57943019e-05  6.63203746e-02 -3.61599191e-03\n",
      "  1.45542559e-34 -2.57070642e-02 -1.21428454e-02 -1.09273922e-02\n",
      "  3.69458906e-02 -2.96491943e-02 -3.72217526e-03  5.72043844e-02\n",
      " -2.55392510e-02 -3.94652635e-02  3.54479514e-02 -8.32363870e-03]\n",
      "\n",
      "Sentence: Hello world!\n",
      "Embedding: [ 1.91737227e-02  2.87365187e-02 -1.23540722e-02  1.58221163e-02\n",
      "  7.90899470e-02 -9.76030622e-03  7.49561656e-03  5.52258939e-02\n",
      "  1.88754573e-02 -2.63798498e-02 -2.68068723e-02 -3.33473124e-02\n",
      " -3.00286394e-02  3.89383473e-02  7.69485012e-02 -7.68075213e-02\n",
      "  6.97796941e-02 -6.93726167e-03 -4.48980629e-02  1.21879410e-02\n",
      "  1.03991190e-02  8.78108013e-03  1.08342441e-02  5.92033751e-02\n",
      "  1.70316510e-02 -2.56329067e-02  9.10408795e-03  3.73561238e-03\n",
      "  3.32367271e-02  5.40162669e-03 -3.08647081e-02  5.92164928e-03\n",
      "  5.06224819e-02  6.56094775e-02  2.12388136e-06 -5.11630736e-02\n",
      "  2.44774260e-02 -7.60704000e-03 -7.54033076e-03  3.69991013e-03\n",
      " -2.33545946e-03  2.81633008e-02 -1.65304653e-02  9.42936260e-03\n",
      "  6.09956868e-03 -4.32090573e-02 -5.53793740e-04 -7.96662457e-03\n",
      "  1.98367350e-02  2.02401187e-02 -4.93254606e-03  1.22366659e-02\n",
      " -6.17670044e-02 -2.02075718e-03  4.50151935e-02  3.01423874e-02\n",
      "  3.64244394e-02  1.64621621e-02  1.17515866e-02 -4.62705977e-02\n",
      "  5.13225645e-02 -3.50426957e-02 -2.48365365e-02 -6.78299321e-03\n",
      " -6.15490775e-04  3.80288623e-02 -2.47982685e-02  3.53081636e-02\n",
      " -7.37576792e-03  6.03849143e-02  3.16642299e-02 -2.13077981e-02\n",
      "  3.91404554e-02  7.03454241e-02  1.12680243e-02 -6.77807331e-02\n",
      " -3.48134041e-02  7.59453699e-02  2.43842825e-02  1.01228086e-02\n",
      " -2.56590880e-02  1.57113057e-02 -1.00361146e-02 -2.50009336e-02\n",
      " -2.85806023e-02  7.33537525e-02  1.09247221e-02 -6.60131080e-03\n",
      "  2.30300426e-02  2.76657473e-02 -1.60924792e-02 -9.57247615e-03\n",
      " -2.69191489e-02  2.79843174e-02  5.70768397e-03 -4.97549213e-02\n",
      "  2.81998180e-02  1.87634695e-02  6.00257926e-02 -6.98942393e-02\n",
      "  5.58036240e-03  4.47613522e-02  1.92690156e-02  1.83421802e-02\n",
      "  1.19074173e-02  2.95257960e-02  1.27076479e-02 -9.85340327e-02\n",
      " -4.09881538e-03 -6.13115402e-03 -3.39137129e-02  1.65321883e-02\n",
      "  5.16282134e-02  1.97650120e-02 -4.58239540e-02 -5.16087690e-04\n",
      "  1.53688770e-02  4.48495038e-02  1.93865225e-02 -4.05020965e-03\n",
      " -6.72020484e-03 -3.42008821e-03  1.66347865e-02  8.94474331e-03\n",
      "  1.62144098e-02 -1.00794956e-02 -2.36646961e-02  1.02804066e-03\n",
      " -4.50740159e-02 -7.11199865e-02 -7.62604736e-03  1.50398782e-03\n",
      "  3.16536054e-02  1.31289642e-02  1.94331892e-02  3.16213258e-02\n",
      " -3.70885804e-02  1.17649045e-02  3.78225408e-02 -3.26599553e-03\n",
      " -3.49017344e-02  1.35727506e-02  1.16819805e-02 -1.83341410e-02\n",
      " -3.90379282e-04 -3.67886300e-04  5.92751354e-02  7.33462200e-02\n",
      "  1.57829057e-02 -1.74836200e-02 -1.02733569e-02 -1.00880563e-02\n",
      " -9.01498124e-02  2.13091867e-03  4.13725935e-02  4.44958024e-02\n",
      "  1.84938256e-02 -3.75403091e-02 -8.33088607e-02 -4.01758179e-02\n",
      " -9.14124493e-03  1.78720485e-02  3.49662192e-02  4.31778049e-03\n",
      "  4.44155559e-02  3.12555730e-02  1.65182278e-02 -3.32651027e-02\n",
      "  1.21721160e-03 -1.08136795e-01 -1.71430930e-02 -7.12404633e-03\n",
      " -3.86399543e-03  2.97890846e-02 -4.40898584e-03 -4.46760505e-02\n",
      " -8.18911195e-02  4.49774899e-02  3.11202519e-02  4.32212924e-04\n",
      " -3.04392371e-02 -9.11770388e-02 -2.08064970e-02 -3.35289091e-02\n",
      " -4.90985103e-02  4.62108813e-02  2.62858756e-02  1.62742082e-02\n",
      " -3.98701988e-02  3.70658971e-02  1.64647605e-02  2.29481384e-02\n",
      " -4.77646776e-02 -2.73195114e-02 -3.84584330e-02  1.27943857e-02\n",
      "  1.61465909e-02 -2.51711905e-02  3.58928107e-02 -8.60399380e-02\n",
      " -2.51331516e-02  5.85450493e-02  1.88994575e-02 -1.10533983e-02\n",
      " -2.88859643e-02  1.97064355e-02  7.14479536e-02 -9.70279239e-03\n",
      "  3.19969617e-02  6.47672564e-02  2.58722994e-03  3.74227902e-03\n",
      "  2.30630543e-02  5.59965856e-02 -5.46831219e-03  8.28384142e-03\n",
      "  1.48229534e-02  6.77185059e-02 -2.22103707e-02  9.26544368e-02\n",
      "  3.09270370e-04  1.13493232e-02  4.54021767e-02 -2.84848846e-02\n",
      "  3.68759222e-02 -5.37904613e-02  1.15620578e-02  2.63585262e-02\n",
      " -3.00342664e-02  2.20608581e-02 -3.82237695e-02 -1.32851768e-02\n",
      "  1.17402533e-02 -3.87181453e-02 -2.11312491e-02 -2.12082230e-02\n",
      "  6.13836572e-02 -2.09865924e-02  4.60760444e-02 -1.40556693e-01\n",
      "  3.14407311e-02  3.47950496e-02  1.34149576e-02 -3.90576757e-02\n",
      "  5.26756532e-02  2.82531977e-02 -7.78554473e-03  2.50696801e-02\n",
      " -1.64742991e-02  1.05086779e-02  1.58723444e-02  3.98024311e-03\n",
      " -9.53608826e-02 -1.39456000e-02 -2.06975825e-03 -4.27397415e-02\n",
      " -4.07640524e-02  2.21872889e-03 -3.38128172e-02 -4.63380553e-02\n",
      "  2.03127973e-02 -3.97232175e-02  2.36008922e-03  3.08819711e-02\n",
      "  4.42394949e-02  2.09607035e-02  3.72630055e-03 -6.12541474e-02\n",
      " -2.98867803e-02 -4.14771074e-03  2.18837671e-02  2.74256617e-02\n",
      "  1.80789344e-02 -4.87662246e-03 -3.81266102e-02 -1.12980260e-02\n",
      "  4.82324362e-02  5.27302315e-03  5.80645865e-03  3.92984003e-02\n",
      "  3.25348042e-02 -8.19188543e-03 -3.23430263e-02  1.33358641e-02\n",
      "  1.63215469e-03  3.75178941e-02  4.17794362e-02 -3.35737839e-02\n",
      "  3.94029804e-02  3.38623812e-03 -4.91535142e-02  5.30147925e-04\n",
      " -3.68258506e-02 -2.79935729e-02  1.96319427e-02 -2.28685830e-02\n",
      "  1.09326504e-02 -6.48552850e-02 -1.26678273e-02 -2.52634268e-02\n",
      " -3.43988091e-02  3.62152494e-02  2.49012653e-02 -1.61011033e-02\n",
      "  3.05041466e-02  7.52815325e-03  1.22987367e-02 -6.00184426e-02\n",
      " -2.96193734e-02  5.00399880e-02  3.96712497e-02 -6.70808135e-03\n",
      " -1.35938236e-02  3.60015668e-02  1.72831882e-02 -1.03489775e-02\n",
      " -3.32858078e-02 -9.80149731e-02  2.04230174e-02 -1.96093041e-02\n",
      "  1.21857291e-02  3.33469436e-02 -2.07347814e-02  2.30167545e-02\n",
      " -4.36715223e-02  8.60218406e-02  2.86370516e-03  4.26322743e-02\n",
      "  4.50773574e-02  9.71537735e-03  2.12641153e-02  1.51655637e-02\n",
      " -2.43581980e-02 -5.37793245e-03 -4.28858027e-02 -5.80270216e-02\n",
      "  2.51577366e-02 -4.96683456e-02 -1.73642896e-02  1.64442956e-02\n",
      " -4.84491438e-02 -5.09288199e-02 -3.81457061e-02  8.45359452e-03\n",
      " -4.36721519e-02 -3.15040909e-02  7.95563962e-03 -2.85016410e-02\n",
      " -7.62111545e-02 -1.97425708e-02 -3.38924490e-02 -5.05803674e-02\n",
      "  1.56861898e-02 -1.03990529e-02 -1.30821913e-02 -3.45803574e-02\n",
      " -1.23005658e-02  8.99058953e-03 -2.08482780e-02 -3.54248695e-02\n",
      " -1.34092513e-02 -4.39252369e-02  3.70026454e-02  8.23190957e-02\n",
      " -6.59840764e-04 -4.02661003e-02  3.09137385e-02  4.47829701e-02\n",
      " -4.74218912e-02 -8.96504801e-03 -6.94045750e-03  4.97259311e-02\n",
      " -1.87435709e-02  5.94354197e-02 -2.65646987e-02  2.17275284e-02\n",
      " -4.95409966e-02 -1.01868184e-02  2.88019981e-02 -9.03274864e-03\n",
      "  3.82599086e-02 -4.37170593e-03  4.64569032e-02 -7.70741003e-03\n",
      " -2.65654698e-02 -2.91740727e-02 -1.09978123e-02  1.44797144e-02\n",
      "  4.97334823e-03  5.80651313e-02  2.29797158e-02 -7.41408095e-02\n",
      "  1.89360343e-02  8.61384999e-03  1.79635105e-03  3.41153704e-02\n",
      " -1.21451332e-04  9.74266510e-03 -4.22965512e-02  2.68055629e-02\n",
      " -2.85971370e-02  2.09479071e-02  7.28383288e-02  2.41208058e-02\n",
      "  6.19545020e-02 -5.87002933e-03  7.47879967e-02  1.24214897e-02\n",
      "  1.19447196e-02 -1.68881658e-03 -3.58084813e-02  1.30942289e-03\n",
      "  1.37494772e-03 -3.13582197e-02 -1.62730962e-02 -6.01772554e-02\n",
      " -2.37561744e-02 -6.51232302e-02 -2.40335204e-02 -9.39345453e-03\n",
      "  1.07824802e-02 -4.09283768e-03  3.66967142e-04  4.73604910e-02\n",
      " -3.64118343e-04  6.08531386e-03  2.88276225e-02  1.13299593e-01\n",
      "  1.95047725e-02  1.21034561e-02 -9.62608680e-03  1.03835817e-02\n",
      "  5.67056052e-02  5.13534471e-02 -6.35670125e-03 -1.92078725e-02\n",
      "  6.95615308e-03 -6.90898299e-03 -6.09237924e-02 -2.48148032e-02\n",
      " -6.31585903e-03 -7.14292191e-03 -1.27549423e-02 -8.85806512e-04\n",
      " -1.93068273e-02  4.31432351e-02  1.02687813e-02 -3.81556489e-02\n",
      "  2.70079076e-02 -7.15295672e-02  2.45865509e-02 -1.61339268e-02\n",
      " -1.77070778e-02 -6.26399815e-02  1.64758898e-02 -7.19615817e-02\n",
      "  8.69687414e-04  2.96784425e-03  3.94573696e-02 -4.71780524e-02\n",
      "  8.78058597e-02  2.12598965e-02 -6.53822273e-02  1.24869449e-02\n",
      "  5.18306717e-02 -1.11386599e-03 -6.35767402e-03  2.98758820e-02\n",
      "  5.15517630e-02 -2.73087202e-03 -5.80668561e-02  6.33830950e-03\n",
      " -2.52553914e-02 -2.79888436e-02 -1.92380976e-02  5.31652756e-02\n",
      " -6.86158752e-03 -4.61083353e-02 -7.04449117e-02 -2.20947042e-02\n",
      " -7.25349644e-04 -1.82642974e-02  8.22455715e-03  5.05146664e-03\n",
      " -2.08496973e-02 -5.82599826e-02  5.17655686e-02 -1.64420661e-02\n",
      " -3.70763987e-02  1.87967736e-02 -1.42325647e-02  8.72147211e-04\n",
      "  6.54906556e-02  3.16015519e-02 -6.53312728e-02  3.46707739e-02\n",
      "  3.84957492e-02 -4.14432399e-02 -1.08024012e-02  4.43097875e-02\n",
      "  5.03092632e-03  3.02684177e-02 -2.50593461e-02 -1.86340534e-03\n",
      "  2.69364659e-02 -9.18120611e-03  1.42919284e-03  2.68609170e-03\n",
      "  1.12633230e-02 -4.95557263e-02  1.31691499e-02  2.07001604e-02\n",
      " -4.18449566e-02 -8.94659907e-02  2.33850181e-02  9.91613567e-02\n",
      " -6.09352603e-04  1.82037298e-02  2.11171098e-02 -5.05483039e-02\n",
      "  3.40849385e-02 -1.32596651e-02 -2.13380177e-02 -1.95383709e-02\n",
      "  2.38800235e-02  1.12978499e-02 -4.00539115e-03 -2.41861586e-02\n",
      "  1.32577848e-02  2.26726779e-03 -5.02162501e-02 -1.11844379e-03\n",
      "  4.78284359e-02  1.90946683e-02 -2.90437210e-02  5.86057529e-02\n",
      "  5.28842211e-02  1.92840230e-02 -3.04640960e-02  8.17226693e-02\n",
      " -2.49547921e-02  3.35668884e-02 -1.17797060e-02  2.69153770e-02\n",
      " -2.51449877e-03  6.32932484e-02  3.30374055e-02 -3.00421123e-03\n",
      " -1.66061651e-02 -3.92380506e-02  3.08186258e-03  2.80030873e-02\n",
      " -8.10903218e-03 -9.33825132e-03 -1.11556882e-02 -7.47020709e-33\n",
      " -3.22054736e-02  3.56099494e-02 -5.82496151e-02 -1.60794910e-02\n",
      " -6.52419552e-02 -3.33213024e-02  2.13987716e-02  1.50862765e-02\n",
      " -1.73138883e-02  4.09016274e-02 -3.87609564e-02  1.05189243e-02\n",
      "  2.50658970e-02  3.13548464e-03 -2.31369883e-02 -4.52394644e-03\n",
      "  3.57131846e-02  4.11871523e-02 -2.86716502e-04  1.74105242e-02\n",
      "  1.22148674e-02  2.46188920e-02  4.59370539e-02 -1.33532863e-02\n",
      "  7.13071078e-02 -2.96964333e-03 -3.59615348e-02 -2.72705369e-02\n",
      "  6.44440353e-02 -1.71524119e-02 -5.55021130e-02  8.80481489e-03\n",
      " -1.35961846e-02  1.90239698e-02 -5.36696520e-03  3.12593654e-02\n",
      " -1.66033618e-02 -1.22872619e-02  5.07900864e-02 -1.03928242e-02\n",
      "  2.11504102e-02  4.54903468e-02 -4.67936918e-02 -4.98828217e-02\n",
      "  1.74908973e-02  3.21025029e-02  4.09144396e-03  2.73100175e-02\n",
      "  3.92666422e-02 -1.01146577e-02 -7.32675940e-02 -7.26894066e-02\n",
      " -1.01326741e-02 -6.41314611e-02 -1.01923183e-01  7.00136367e-03\n",
      " -5.76175796e-03 -6.46821782e-03  2.20477507e-02  1.01690879e-02\n",
      "  6.98977709e-02 -4.26164940e-02 -4.28642593e-02  4.30843569e-02\n",
      "  1.58814993e-02  7.93900415e-02  1.16006039e-01  2.20975354e-02\n",
      " -3.04973070e-02 -7.12646842e-02 -3.90831567e-02 -3.21931997e-03\n",
      " -4.58594374e-02  4.95482460e-02 -3.22161131e-02 -9.35567892e-04\n",
      " -6.84288442e-02 -3.27660255e-02 -7.94433150e-03 -3.21570635e-02\n",
      " -3.96366604e-02 -1.84605960e-02 -1.14914244e-02  2.94621736e-02\n",
      "  1.94462240e-02  4.15210873e-02 -1.90556201e-03  2.43717860e-02\n",
      " -5.03088720e-02  1.94552299e-02 -2.26781685e-02 -6.72485679e-02\n",
      "  2.77108848e-02 -5.28803840e-02 -1.71071030e-02 -2.00080313e-02\n",
      "  4.75357845e-02 -1.16792396e-02 -2.23239115e-03 -1.98743381e-02\n",
      "  3.11620068e-02 -6.44605933e-03 -5.14813699e-02  3.10154874e-02\n",
      "  4.21764283e-03  5.90362251e-02 -5.95618002e-02 -1.20727634e-02\n",
      " -5.79645224e-02 -1.52969249e-02  1.19450353e-02  9.56440170e-04\n",
      " -7.11028057e-04  9.00591444e-03 -1.07603036e-02  4.16438188e-03\n",
      "  1.98662169e-02  1.48234365e-03 -4.13043313e-02 -2.06007119e-02\n",
      "  3.54354493e-02 -2.52422169e-02 -4.03880775e-02  1.35236429e-02\n",
      " -3.02803665e-02 -1.57544073e-02 -6.40298240e-03  2.88539343e-02\n",
      "  3.77058797e-02 -1.57545321e-02  1.93659235e-02 -3.18935253e-02\n",
      "  2.74357888e-07  1.36573622e-02 -2.96957269e-02 -1.86281670e-02\n",
      "  4.90262806e-02  1.95595566e-02  6.26195781e-03 -2.18236893e-02\n",
      "  7.88233876e-02  5.18120006e-02  1.02162594e-02  1.23885674e-02\n",
      " -7.14072492e-03 -1.12918746e-02 -7.13305548e-02 -5.56696355e-02\n",
      "  5.20572066e-03 -5.79465181e-02 -1.12188600e-01 -2.72846613e-02\n",
      "  1.57684702e-02 -1.67696476e-02  5.12126321e-03 -4.01668288e-02\n",
      " -1.31393084e-03  2.60912273e-02 -2.37526279e-02  3.94152887e-02\n",
      " -3.82060069e-03 -6.60818908e-03  9.01801977e-03 -3.99146602e-02\n",
      " -4.71641086e-02  2.90916506e-02 -3.72883491e-02  1.84368845e-02\n",
      " -2.63715331e-02  1.30457589e-02  2.67634429e-02  6.50379667e-03\n",
      "  8.21977630e-02 -3.83655876e-02  3.92348319e-02 -2.39916816e-02\n",
      " -3.60896997e-02  1.78224184e-02  1.22119738e-02  3.41273881e-02\n",
      "  8.46595410e-03 -2.43397951e-02 -4.21196409e-02 -1.19034331e-02\n",
      " -2.82946881e-02  7.91268796e-03  1.75073314e-02 -1.20987371e-02\n",
      "  2.61468105e-02  6.27233908e-02  2.33795736e-02  3.18711959e-02\n",
      " -4.35521565e-02  2.96292291e-03 -1.10605143e-01 -5.36748627e-03\n",
      "  2.03249566e-02 -8.06447212e-03 -1.79817658e-02 -2.39729807e-02\n",
      "  1.15365070e-34 -1.68201979e-02  2.72667892e-02  2.62049641e-02\n",
      "  6.68589100e-02 -1.96504481e-02 -9.88799613e-04 -4.00086753e-02\n",
      "  1.52709633e-02  4.57853153e-02 -3.14779617e-02 -1.91929638e-02]\n"
     ]
    }
   ],
   "source": [
    "# Sentence Transformers is an open-source library for interfacing with embedding models\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", device=\"cpu\")\n",
    "sentences = [\"This is for a tech talk\", \"Hello world!\"]\n",
    "\n",
    "embeddings = embedding_model.encode(sentences)\n",
    "embeddings_dict = dict(zip(sentences, embeddings))\n",
    "\n",
    "# See the embeddings\n",
    "for sentence, embedding in embeddings_dict.items():\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Embedding: {embedding}\")\n",
    "    print(\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-20T10:26:51.804076Z",
     "start_time": "2024-11-20T10:26:50.396915Z"
    }
   },
   "id": "f4ef2b8c9a77f1c4",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(768,)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at the first embedding\n",
    "embeddings[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-20T10:26:51.808258Z",
     "start_time": "2024-11-20T10:26:51.805306Z"
    }
   },
   "id": "de18448d6ab95e12",
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "source": [
    "So, we are now representing each sentence with 768 numbers. Or another way of putting this is this vector now represents a point 768-dimensional space, where each dimension encapsulates a certain feature or characteristic of the data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97e0e3acbbac0717"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Efficiency of Vector Operations: CPU vs GPU\n",
    "\n",
    "In the previous example, we performed the operation on the CPU. CPUs are designed for sequential operations (one after the next). Let's see how long it takes to embed all of our FAQs using this approach:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58e4e175612742be"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.29 s, sys: 65.1 ms, total: 5.35 s\n",
      "Wall time: 671 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "embedding_model.to(\"cpu\")\n",
    "\n",
    "# Embed each FAQ one by one\n",
    "for faq in faqs:\n",
    "    # Encode the FAQ text\n",
    "    embedding = embedding_model.encode(faq['faq'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-20T10:26:52.494242Z",
     "start_time": "2024-11-20T10:26:51.809162Z"
    }
   },
   "id": "90f5347fe45ec18f",
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's try on the GPU. I have an NVIDIA RTX 3070, which is by no means necessary the fastest consumer card out there, but let's see how it fares in comparison..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9588bb7c3af9feb0"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.49 s, sys: 26.3 ms, total: 1.52 s\n",
      "Wall time: 271 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "embedding_model.to(\"cuda\")\n",
    "\n",
    "# Embed each FAQ one by one\n",
    "for faq in faqs:\n",
    "    # Encode the FAQ text\n",
    "    embedding = embedding_model.encode(faq['faq'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-20T10:26:52.769213Z",
     "start_time": "2024-11-20T10:26:52.495764Z"
    }
   },
   "id": "412349e141f48c81",
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wow, ~35x faster. But wait, we're still doing this sequentially due to nature of the code... Can we get some more performance out of my GPU?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6e173c6257106d4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35.5 ms, sys: 1.18 ms, total: 36.7 ms\n",
      "Wall time: 25.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "faq_texts = [faq['faq'] for faq in faqs]\n",
    "batch_embeddings = embedding_model.encode(faq_texts, batch_size=32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-20T10:26:52.797440Z",
     "start_time": "2024-11-20T10:26:52.770033Z"
    }
   },
   "id": "15c5de0181680f4e",
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "source": [
    "Another ~8x improvement, leading to a total ~301x improvement overall in comparison to using the CPU for our operations.\n",
    "\n",
    "But why?\n",
    "\n",
    "GPUs excel at tasks such as this because they are designed for parallel computing, allowing them to process thousands of vector operations simultaneously, whereas CPUs are designed for sequential processing. For example, think of a video game and the amount of polygons the GPU has to render each frame. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc1a4d6c1562148f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Typically to store these embeddings, we would use a Vector Database, such as Pinecone, Elasticsearch, Qdrant, etc. For now though, let's just store them in a NumPy array\n",
    "import numpy as np\n",
    "\n",
    "# Add the embedding to each FAQ object\n",
    "for faq, embedding in zip(faqs, batch_embeddings):\n",
    "    faq['embedding'] = embedding\n",
    "\n",
    "# Extract the embeddings from each FAQ and store them in a NumPy array\n",
    "embeddings_array = np.array([faq['embedding'] for faq in faqs])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-20T10:26:52.800833Z",
     "start_time": "2024-11-20T10:26:52.798332Z"
    }
   },
   "id": "251153aa56ad360f",
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ***Retrieval*** Augmented Generation\n",
    "\n",
    "The first phase of RAG is Retrieval. This includes gathering relevant data based on some input query. For this, we have a few options:\n",
    "\n",
    "- We can do this by Similarity Search, otherwise known as Vector Search or Semantic Search. These are techniques that use embeddings to understand the meaning and context of queries and data, enabling more accurate and relevant search results by comparing vector representations rather than relying on exact keyword matches.\n",
    "- Another option is Keyword Search, where based on some input query, we find relevant data that contains that input. For example if I were to search for \"Apple\", it would return passages that also contain \"Apple\".\n",
    "\n",
    "In our instance, we want to ask some question and then retrieve relevant FAQs so our LLM has context to answer accurately. Let's go for Semantic Search, as the user might ask their question in a way that Keyword Search wouldn't pick up the relevant passages we need."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "547705007e4acd8b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([20, 768])"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take our NumPy array and convert it into a Tensor using PyTorch\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Convert the NumPy array of embeddings to a PyTorch tensor\n",
    "embeddings_tensor = torch.tensor(embeddings_array).to(device)\n",
    "\n",
    "# Print the shape of the resulting tensor to verify\n",
    "embeddings_tensor.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-20T10:26:52.820135Z",
     "start_time": "2024-11-20T10:26:52.801771Z"
    }
   },
   "id": "df25d50971a58f09",
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": [
    "This means my Tensor has 20 vectors, each with 768 elements.\n",
    "\n",
    "But what is PyTorch, and what is a Tensor? \n",
    "\n",
    "PyTorch is an open-source deep learning framework that provides tools for building and training neural networks.\n",
    "\n",
    "A Tensor is a multidimensional array similar to a NumPy array but with additional capabilities, such as running on GPUs for accelerated computations. While a vector is a 1-dimensional tensor (or array), PyTorch tensors can represent data in higher dimensions (like matrices or multidimensional arrays), which is essential for deep learning tasks."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f03990810e6475c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have our embeddings stored, let's create a small Semantic Search pipeline by:\n",
    "1. Defining a query string.\n",
    "2. Transforming the query string into an embedding.\n",
    "3. Performing a dot product or cosine similarity operation (more on this soon) between the FAQ embeddings and the query embedding\n",
    "4. Getting the top 3 matching results, with the most similar being first"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16a1f9ce488b99c9"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAQ: What should I do if I receive a damaged item? Please contact our customer service immediately with your order number and a photo of the damaged item.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# It's important to embed our query using the same model we used to embed our FAQs\n",
    "query_embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", device=device)\n",
    "\n",
    "# 1. Define the query\n",
    "query = \"item is damaged\"\n",
    "\n",
    "# 2. Embed the query using the same model we used to embed FAQs\n",
    "query_embedding = query_embedding_model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "# 3. Get similarity scores with the dot product (use cosine similarity if outputs of model aren't normalized, they are in our case).\n",
    "dot_scores = util.dot_score(a=query_embedding, b=embeddings_tensor)\n",
    "\n",
    "# 4. Get top-k results (3 for now)\n",
    "top_faqs = torch.topk(dot_scores, k=3)\n",
    "\n",
    "# Extract the index of the top scores\n",
    "top_indices = top_faqs.indices[0].tolist()\n",
    "\n",
    "# Retrieve and print the most relevant FAQ by looking at the index at the first index in top_indices\n",
    "relevant_faq = faqs[top_indices[0]]\n",
    "print(f\"FAQ: {relevant_faq['faq']}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-20T10:26:53.959411Z",
     "start_time": "2024-11-20T10:26:52.820888Z"
    }
   },
   "id": "a7e94b7be83fe93e",
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note: To use dot product for comparison, ensure vector sizes are of same shape (e.g. 768) and tensors/vectors are in the same datatype (e.g. both are torch.float32)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e5afda28aab1a10"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Similarity Measures: Dot Product and Cosine Similarity\n",
    "\n",
    "Two of the most common similarity measures between vectors are dot product and cosine similarity.\n",
    "In essence, closer vectors will have higher scores, further away vectors will have lower scores.\n",
    "The \"scores\" however are calculated and are represented different across both approaches."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e2939068b33a38"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product between vector1 and vector2 (normalized): tensor(14.)\n",
      "Dot product between vector1 and vector3 (normalized): tensor(32.)\n",
      "Dot product between vector1 and vector4 (normalized): tensor(-14.)\n",
      "Cosine similarity between vector1 and vector2: tensor(1.0000)\n",
      "Cosine similarity between vector1 and vector3: tensor(0.9746)\n",
      "Cosine similarity between vector1 and vector4: tensor(-1.0000)\n"
     ]
    }
   ],
   "source": [
    "def normalize(vector):\n",
    "    \"\"\"\n",
    "    To normalize a vector means for it to have magnitude (length) of 1. \n",
    "    The magnitude of a vector is determined by calculating the distance between the origin of the space the vector is in (0, 0,..., 0) to the space the vector occupies.\n",
    "    This is done by dividing each component of the vector by its Euclidean (L2) norm.\n",
    "    After normalization, the vector points in the same direction but has a magnitude of 1.\n",
    "    \"\"\"\n",
    "    norm = torch.sqrt(torch.sum(vector**2))\n",
    "    return vector / norm\n",
    "\n",
    "def dot_product(vector1, vector2):\n",
    "    return torch.dot(vector1, vector2)\n",
    "\n",
    "# If the embeddings are already normalized, then don't use this, as it's just performing an unnecessary computation to try and normalize them again\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    # Normalize the input vectors\n",
    "    normalized_vector1 = normalize(vector1)\n",
    "    normalized_vector2 = normalize(vector2)\n",
    "    # Compute the dot product of the normalized vectors\n",
    "    return torch.dot(normalized_vector1, normalized_vector2)\n",
    "\n",
    "# Example vectors/tensors\n",
    "vector1 = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "vector2 = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "vector3 = torch.tensor([4, 5, 6], dtype=torch.float32)\n",
    "vector4 = torch.tensor([-1, -2, -3], dtype=torch.float32)\n",
    "\n",
    "# Calculate dot product. Note: these vectors aren't normalized, so we're also taking into account the magnitude of each.\n",
    "print(\"Dot product between vector1 and vector2 (normalized):\", dot_product(vector1, vector2))\n",
    "print(\"Dot product between vector1 and vector3 (normalized):\", dot_product(vector1, vector3))\n",
    "print(\"Dot product between vector1 and vector4 (normalized):\", dot_product(vector1, vector4))\n",
    "\n",
    "# Cosine similarity\n",
    "print(\"Cosine similarity between vector1 and vector2:\", cosine_similarity(vector1, vector2))\n",
    "print(\"Cosine similarity between vector1 and vector3:\", cosine_similarity(vector1, vector3))\n",
    "print(\"Cosine similarity between vector1 and vector4:\", cosine_similarity(vector1, vector4))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-20T11:01:08.937405Z",
     "start_time": "2024-11-20T11:01:08.931318Z"
    }
   },
   "id": "75f5aad55c0b8935",
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Key Takeaways:\n",
    "Think of a vector as an arrow. Its Direction tells you where the arrow is pointing. Magnitude tells you how long the arrow is. \n",
    "\n",
    "### How Dot Product and Cosine Similarity Differ:\n",
    "The two metrics differ only if the vectors are not normalized.\n",
    "Dot Product takes into account both magnitude and direction of the vectors. Larger vectors (higher magnitudes) will yield a larger dot product, even if their directions are identical.\n",
    "Cosine Similarity normalizes the vectors first, effectively removing the influence of magnitude and focuses purely on the directional alignment between the vectors."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5679571d0b27d8bd"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 norm of the first FAQ's embedding: 0.9999999403953552\n",
      "Is the embedding normalized? Yes\n"
     ]
    }
   ],
   "source": [
    "# Get the embedding of the first FAQ\n",
    "first_faq_embedding = embeddings_tensor[0]\n",
    "\n",
    "# Calculate the L2 norm of the embedding\n",
    "embedding_norm = torch.linalg.vector_norm(first_faq_embedding, ord=2)\n",
    "\n",
    "# Check if the embedding is normalized (L2 norm should be approximately 1)\n",
    "is_normalized = torch.isclose(embedding_norm, torch.tensor(1.0), atol=1e-6)\n",
    "\n",
    "# Print the results\n",
    "print(f\"L2 norm of the first FAQ's embedding: {embedding_norm.item()}\")\n",
    "print(f\"Is the embedding normalized? {'Yes' if is_normalized else 'No'}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-20T10:35:23.756720Z",
     "start_time": "2024-11-20T10:35:23.753094Z"
    }
   },
   "id": "f4eb9e0291ba4554",
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "source": [
    "# What next?\n",
    "\n",
    "Now we have our relevant documents, we can now pass them alongside the user's query to the LLM\n",
    "\n",
    "![Generation](generation.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44be03afc727418e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
